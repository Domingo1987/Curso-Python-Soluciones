import requests
from bs4 import BeautifulSoup
import csv
import re
import time
from datetime import datetime

def extraer_articulo(numero):
    """
    Extrae un art√≠culo espec√≠fico de la Constituci√≥n de Uruguay
    """
    base_url = "https://www.impo.com.uy/bases/constitucion/1967-1967"
    url = f"{base_url}/{numero}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Inicializar variables
        fecha = ""
        articulo_num = f"Art√≠culo {numero}"
        descripcion = ""
        notas = ""
        
        # 1. EXTRAER FECHA
        # Buscar patr√≥n: <h5>Fecha de Publicaci√≥n: DD/MM/YYYY </h5>
        fecha_element = soup.find('h5', string=re.compile(r'Fecha de Publicaci√≥n', re.IGNORECASE))
        if fecha_element:
            fecha_texto = fecha_element.get_text()
            match_fecha = re.search(r'(\d{2}/\d{2}/\d{4})', fecha_texto)
            if match_fecha:
                fecha = match_fecha.group(1)
        
        # Si no encuentra en h5, buscar en cualquier lugar
        if not fecha:
            fecha_general = soup.find(text=re.compile(r'Fecha de Publicaci√≥n.*?\d{2}/\d{2}/\d{4}', re.IGNORECASE))
            if fecha_general:
                match_fecha = re.search(r'(\d{2}/\d{2}/\d{4})', fecha_general)
                if match_fecha:
                    fecha = match_fecha.group(1)
        
        # 2. EXTRAER T√çTULO DEL ART√çCULO
        # Buscar <h4 class="resultado">Art√≠culo X</h4>
        titulo_element = soup.find('h4', {'class': 'resultado'})
        if titulo_element:
            titulo_texto = titulo_element.get_text().strip()
            if 'Art√≠culo' in titulo_texto:
                articulo_num = titulo_texto
        
        # 3. EXTRAER DESCRIPCI√ìN
        # Buscar el <pre> que viene despu√©s del h4 del art√≠culo
        if titulo_element:
            descripcion_element = titulo_element.find_next_sibling('pre')
            if descripcion_element:
                descripcion = descripcion_element.get_text().strip()
                # Limpiar espacios extra
                descripcion = re.sub(r'\s+', ' ', descripcion)
        
        # Si no encuentra descripci√≥n con el m√©todo anterior, buscar por patr√≥n
        if not descripcion:
            # Buscar todos los <pre> y encontrar el que contiene el texto del art√≠culo
            pre_elements = soup.find_all('pre')
            for pre in pre_elements:
                texto = pre.get_text().strip()
                # Verificar que no sea una nota (no contiene "Notas:" o asteriscos al inicio)
                if (texto and 
                    len(texto) > 20 and 
                    not re.match(r'^\(\*\)', texto) and 
                    'Notas:' not in texto and
                    'Ver en esta norma' not in texto):
                    descripcion = re.sub(r'\s+', ' ', texto)
                    break
        
        # 4. EXTRAER NOTAS
        # Buscar <pre class="italica"> que contiene las notas
        notas_elements = soup.find_all('pre', {'class': 'italica'})
        notas_lista = []
        
        for nota_element in notas_elements:
            nota_texto = nota_element.get_text().strip()
            
            # Filtrar l√≠neas de separaci√≥n (solo guiones o asteriscos)
            if re.match(r'^[\-\*\(\)]*$', nota_texto):
                continue
                
            # Limpiar formato de notas
            nota_texto = re.sub(r'^\(\*\)\s*', '', nota_texto)  # Remover (*) al inicio
            nota_texto = re.sub(r'^Notas:\s*', '', nota_texto, flags=re.IGNORECASE)  # Remover "Notas:"
            nota_texto = re.sub(r'\s+', ' ', nota_texto)  # Limpiar espacios
            
            if nota_texto and len(nota_texto) > 3:
                notas_lista.append(nota_texto)
        
        # Tambi√©n buscar notas en elementos que contengan "Ver en esta norma" o enlaces
        enlaces_notas = soup.find_all(text=re.compile(r'Ver en esta norma|art√≠culo:', re.IGNORECASE))
        for enlace in enlaces_notas:
            parent = enlace.parent
            if parent:
                nota_texto = parent.get_text().strip()
                nota_texto = re.sub(r'\s+', ' ', nota_texto)
                if nota_texto and nota_texto not in notas_lista:
                    notas_lista.append(nota_texto)
        
        # Unir todas las notas
        notas = ' | '.join(notas_lista) if notas_lista else ""
        
        # Validar que tenemos informaci√≥n √∫til
        if not descripcion or len(descripcion) < 10:
            print(f"‚ö†Ô∏è  Art√≠culo {numero}: descripci√≥n muy corta o vac√≠a")
            return None
            
        return {
            'fecha': fecha,
            'articulo': articulo_num,
            'descripcion': descripcion,
            'notas': notas
        }
        
    except requests.RequestException as e:
        print(f"‚ùå Error de conexi√≥n en art√≠culo {numero}: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Error procesando art√≠culo {numero}: {e}")
        return None

def extraer_constitucion_completa():
    """
    Extrae todos los art√≠culos de la Constituci√≥n de Uruguay (1-332)
    """
    articulos = []
    errores = []
    
    print("üá∫üáæ Iniciando extracci√≥n de la Constituci√≥n de Uruguay...")
    print("üìÑ Extrayendo: fecha, art√≠culo, descripci√≥n y notas")
    print("‚è±Ô∏è  Esto puede tomar varios minutos...\n")
    
    # Rango de art√≠culos (1 a 332)
    for numero in range(1, 333):
        print(f"üìñ Procesando art√≠culo {numero:3d}/332...", end=' ')
        
        articulo = extraer_articulo(numero)
        
        if articulo:
            articulos.append(articulo)
            print("‚úÖ")
        else:
            errores.append(numero)
            print("‚ùå")
        
        # Pausa para no sobrecargar el servidor
        time.sleep(0.8)
        
        # Mostrar progreso cada 25 art√≠culos
        if numero % 25 == 0:
            porcentaje = (numero / 332) * 100
            print(f"\nüìä Progreso: {numero}/332 ({porcentaje:.1f}%)")
            print(f"‚úÖ Extra√≠dos: {len(articulos)} | ‚ùå Errores: {len(errores)}\n")
    
    print(f"\n{'='*60}")
    print(f"üéØ RESUMEN FINAL:")
    print(f"üìù Total art√≠culos procesados: 332")
    print(f"‚úÖ Art√≠culos extra√≠dos exitosamente: {len(articulos)}")
    print(f"‚ùå Art√≠culos con errores: {len(errores)}")
    
    if errores:
        print(f"üîç Art√≠culos problem√°ticos: {errores[:15]}{'...' if len(errores) > 15 else ''}")
    
    # Guardar resultados en CSV
    if articulos:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        nombre_archivo = f'constitucion_uruguay_{timestamp}.csv'
        
        with open(nombre_archivo, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['fecha', 'articulo', 'descripcion', 'notas']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            writer.writeheader()
            for articulo in articulos:
                writer.writerow(articulo)
        
        print(f"\nüíæ Datos guardados en: {nombre_archivo}")
        
        # Mostrar muestra de los primeros art√≠culos
        print(f"\n{'='*60}")
        print("üìã MUESTRA DE ART√çCULOS EXTRA√çDOS:")
        for i, art in enumerate(articulos[:3]):
            print(f"\nüìÖ Fecha: {art['fecha']}")
            print(f"üìÑ {art['articulo']}")
            print(f"üìù Descripci√≥n: {art['descripcion'][:150]}{'...' if len(art['descripcion']) > 150 else ''}")
            if art['notas']:
                print(f"üìå Notas: {art['notas'][:100]}{'...' if len(art['notas']) > 100 else ''}")
            print("-" * 40)
        
        # Estad√≠sticas detalladas
        print(f"\n{'='*60}")
        print("üìà ESTAD√çSTICAS DETALLADAS:")
        
        # Art√≠culos con fecha
        articulos_con_fecha = sum(1 for art in articulos if art['fecha'])
        print(f"üìÖ Art√≠culos con fecha: {articulos_con_fecha}/{len(articulos)}")
        
        # Art√≠culos con notas
        articulos_con_notas = sum(1 for art in articulos if art['notas'])
        print(f"üìå Art√≠culos con notas: {articulos_con_notas}/{len(articulos)}")
        
        # Longitudes de descripci√≥n
        longitudes = [len(art['descripcion']) for art in articulos if art['descripcion']]
        if longitudes:
            print(f"üìè Longitud promedio descripci√≥n: {sum(longitudes) // len(longitudes)} caracteres")
            print(f"üìè Descripci√≥n m√°s larga: {max(longitudes)} caracteres")
            print(f"üìè Descripci√≥n m√°s corta: {min(longitudes)} caracteres")
        
        # Fechas √∫nicas
        fechas_unicas = set(art['fecha'] for art in articulos if art['fecha'])
        print(f"üìÜ Fechas de publicaci√≥n encontradas: {len(fechas_unicas)}")
        if fechas_unicas:
            print(f"üìÜ Fechas: {sorted(fechas_unicas)}")
        
    else:
        print("\n‚ùå No se pudieron extraer art√≠culos. Verifica la conexi√≥n y la estructura del sitio.")
    
    return articulos, errores

def reintentar_errores(numeros_error):
    """
    Reintenta extraer art√≠culos que fallaron en el primer intento
    """
    if not numeros_error:
        return []
    
    print(f"\nüîÑ Reintentando {len(numeros_error)} art√≠culos que fallaron...")
    articulos_recuperados = []
    
    for numero in numeros_error:
        print(f"üîÑ Reintentando art√≠culo {numero}...", end=' ')
        time.sleep(2)  # Pausa m√°s larga para reintentos
        
        articulo = extraer_articulo(numero)
        if articulo:
            articulos_recuperados.append(articulo)
            print("‚úÖ")
        else:
            print("‚ùå")
    
    print(f"üéØ Art√≠culos recuperados en el reintento: {len(articulos_recuperados)}")
    return articulos_recuperados

if __name__ == "__main__":
    print("üöÄ EXTRACTOR DE CONSTITUCI√ìN DE URUGUAY")
    print("=" * 60)
    
    # Extraer todos los art√≠culos
    articulos, errores = extraer_constitucion_completa()
    
    # Reintentar art√≠culos que fallaron
    if errores and len(errores) <= 20:  # Solo reintentar si hay pocos errores
        respuesta = input(f"\n‚ùì ¬øDeseas reintentar los {len(errores)} art√≠culos que fallaron? (s/n): ")
        if respuesta.lower() in ['s', 'si', 's√≠', 'y', 'yes']:
            articulos_recuperados = reintentar_errores(errores)
            
            if articulos_recuperados:
                # Agregar art√≠culos recuperados al CSV m√°s reciente
                import glob
                archivos_csv = glob.glob('constitucion_uruguay_*.csv')
                if archivos_csv:
                    archivo_mas_reciente = max(archivos_csv)
                    with open(archivo_mas_reciente, 'a', newline='', encoding='utf-8') as csvfile:
                        fieldnames = ['fecha', 'articulo', 'descripcion', 'notas']
                        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                        for articulo in articulos_recuperados:
                            writer.writerow(articulo)
                    
                    print(f"‚úÖ Se agregaron {len(articulos_recuperados)} art√≠culos recuperados al CSV.")
    
    print(f"\nüéâ ¬°Proceso completado!")
    print("üìÅ Revisa el archivo CSV generado para ver todos los art√≠culos extra√≠dos.")